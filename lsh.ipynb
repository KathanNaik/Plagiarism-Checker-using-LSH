{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IMPORT MODULES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DEFINING CONSTANTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the Dataset\n",
    "ASSETS_LOCATION = \"dataset\"\n",
    "\n",
    "# SHINGLING\n",
    "SHINGLE_SIZE = 9  # Size of the shingles\n",
    "\n",
    "# MIN HASHING\n",
    "NO_OF_HASH_FUNCTIONS = 100  # Number of min hash functions we are using\n",
    "HASH_MOD = 100003\n",
    "\n",
    "# LOCALITY SENSITIVE HASHING (LSH)\n",
    "CONSTANT_BAND = 20  # Number of Bands\n",
    "CONSTANT_ROW = 2 # Number of Rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SHINGLING OPERATION FUNCTIONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GENERAL SHINGLING FUNCTIONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This function returns a set of k-shingles for the string passed as argument.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findShingles(docData):\n",
    "    shingles = []\n",
    "    for i in range(0, len(docData) - SHINGLE_SIZE + 1):\n",
    "        shingles.append(docData[i:i + SHINGLE_SIZE])\n",
    "    \n",
    "    return set(shingles) # set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SHINGLING OF CORPUS FUNCTIONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This function returns Vocabulary of shingles for entire corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createShinglesVocab():\n",
    "    dataFolder = os.listdir(ASSETS_LOCATION)\n",
    "    dataFolder.sort()\n",
    "\n",
    "    nameOfDocuments = []\n",
    "    vocab = set()\n",
    "\n",
    "    print(\"[.] Creating Shingles for the corpus!\")\n",
    "\n",
    "    for doc in dataFolder:\n",
    "        fileName = ASSETS_LOCATION + f'/{doc}'\n",
    "        nameOfDocuments.append(doc)\n",
    "        filePtr = open(fileName, 'r', encoding='utf-8')\n",
    "        docData = filePtr.read()\n",
    "        docVocab = findShingles(docData)\n",
    "        filePtr.close()\n",
    "        print(doc + \": File has been shingled successfully!\")\n",
    "        vocab.update(docVocab)\n",
    "    \n",
    "\n",
    "    print(\"[+] Shingles Vocabulary created successfully!\")\n",
    "\n",
    "    \n",
    "    # filePtr = open(\"vocab.pkl\", \"wb\")\n",
    "    # pickle.dump(vocab, filePtr)\n",
    "\n",
    "    # filePtr = open(\"nameOfDocuments.pkl\", \"wb\")\n",
    "    # pickle.dump(nameOfDocuments, filePtr)\n",
    "\n",
    "\n",
    "    return nameOfDocuments, vocab # list, set\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This Function assigns a unique id to each shingle of the Vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignIdToShingles(vocab):\n",
    "    dictShinglesId = {}\n",
    "    id = 0\n",
    "    for shingle in vocab:\n",
    "        dictShinglesId[shingle] = id\n",
    "        id += 1\n",
    "\n",
    "    return dictShinglesId # dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating Shingles Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createShinglesMatrix(dictShinglesId):\n",
    "    shingleMatrix = []\n",
    "    \n",
    "    print(\"[.] Creating Shingle Matrix...\")\n",
    "    dataFolder = os.listdir(ASSETS_LOCATION)\n",
    "    dataFolder.sort()\n",
    "    shingleMatrix = []\n",
    "\n",
    "    for doc in dataFolder:\n",
    "        fileName = ASSETS_LOCATION + f'/{doc}'\n",
    "        filePtr = open(fileName, 'r', encoding='utf-8')\n",
    "        docData = filePtr.read()\n",
    "        docVocab = findShingles(docData)\n",
    "        filePtr.close()\n",
    "\n",
    "        docRow = []\n",
    "\n",
    "        for item in docVocab:\n",
    "            docRow.append(dictShinglesId[item])\n",
    "        \n",
    "        docRow.sort()\n",
    "\n",
    "        shingleMatrix.append(docRow)    \n",
    "\n",
    "    print(\"[+] Shingle Matrix Created Successfully!\")\n",
    "\n",
    "    return shingleMatrix # Matrix (where row = number of docs and cols = shingles id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SHINGLING OF QUERY**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating Query Matirx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createQueryMatrix(userQuery, dictShinglesId):\n",
    "    # Generating k-Shingles for the userQuery\n",
    "    setQueryShingles = findShingles(userQuery) # set\n",
    "\n",
    "    listQueryShinglesId = []\n",
    "\n",
    "    for shingle in setQueryShingles:\n",
    "        if shingle in dictShinglesId.keys():\n",
    "            listQueryShinglesId.append(dictShinglesId[shingle])\n",
    "\n",
    "    matrixQueryShinglesId = []\n",
    "    listQueryShinglesId.sort()\n",
    "    matrixQueryShinglesId.append(listQueryShinglesId)\n",
    "\n",
    "    return matrixQueryShinglesId # matrix\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MIN HASH FUNCTIONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generating Random Hash Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This function generates {NO_OF_HASH_FUNCTIONS} random hash function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRandomMinHashFunctions():\n",
    "    hashFunctions = []\n",
    "\n",
    "    random.seed(25)\n",
    "\n",
    "    # Linear Hash Function =>> ax+b\n",
    "    # coefficient = [a,b]\n",
    "    for id in range(0, NO_OF_HASH_FUNCTIONS):\n",
    "        a = random.randint(1, 100000)\n",
    "        b = random.randint(1, 100000)\n",
    "        coefficient = [a, b]\n",
    "\n",
    "        hashFunctions.append(coefficient)\n",
    "    \n",
    "    return hashFunctions # list of min Hash functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Min Hash Technique Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generating Signature Matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Return Matrix of size nxm (n,m are passed as parameters) with all values as INF (very large)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intitlizeMatrixWithInfinity(numberOfDocs):\n",
    "    matrix = []\n",
    "    for i in range(NO_OF_HASH_FUNCTIONS):\n",
    "        row = []\n",
    "        for j in range(numberOfDocs):\n",
    "            row.append(HASH_MOD)  \n",
    "        matrix.append(row) \n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This function generates signature matrix by using min hashing technique using {NO_OF_HASH_FUNCTIONS}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSignatureMatrix(shingleMatrix):\n",
    "    # Stores Random Min Hash Fucntions\n",
    "    hashFunction = generateRandomMinHashFunctions()\n",
    "\n",
    "    numberOfDocs = len(shingleMatrix)\n",
    "    print(numberOfDocs)\n",
    "    # Initilizing all the rows and cols of signatureMatrix with INFINITY (Very Large Value)\n",
    "    signatureMatrix = intitlizeMatrixWithInfinity(numberOfDocs)\n",
    "\n",
    "    # Min Hash Algorithm\n",
    "    print(\"[.] Processing Signature Matrix...\")\n",
    "    for i in range(len(shingleMatrix)):\n",
    "        for j in range(NO_OF_HASH_FUNCTIONS):\n",
    "            # a and b are the constants of the min hash function\n",
    "            a = hashFunction[j][0]\n",
    "            b = hashFunction[j][1]\n",
    "\n",
    "            for k in shingleMatrix[i]:\n",
    "                # a * x + b is the hash function where a and b are constants we generated during random min hash function generator while x is a variable which take the value to be hashed.\n",
    "                hashKey = ((a * (k + 1)) + b) % HASH_MOD\n",
    "                if hashKey < signatureMatrix[j][i]:\n",
    "                    signatureMatrix[j][i] = hashKey\n",
    "\n",
    "    print(\"[+] Signature Matrix Creating Successfull\")\n",
    "\n",
    "    '''\n",
    "    Signature Matrix is a NO_OF_HASH_FUNCTION x NO_OF_DOCUMENTS sized matrix where each cell stores the hash value.\n",
    "    '''\n",
    "\n",
    "    return signatureMatrix # Matrix (rows = number of hash functions & cols = no of docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LSH: Locality Sensitive Hashing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LSH Implementation Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsh(signatureMatrix):\n",
    "    '''\n",
    "    Input: Signature Matrix, number of bands & number of rows.\n",
    "    We perform LSH on the signature matrix by divinding the signature matrix in b bands where each bans contains r rows.    \n",
    "    '''\n",
    "\n",
    "    print(\"[.] LSH of Signature Matrix Started...\")\n",
    "\n",
    "    bucketForBands = {} # bucket (dictinory) that stores sub buckets for all band\n",
    "    numberOfDocuments = len(signatureMatrix[0])\n",
    "\n",
    "    for bandB in range(0, CONSTANT_BAND):\n",
    "        bucketForBandB = {}\n",
    "\n",
    "        for docNumber in range(numberOfDocuments):\n",
    "            hashVector = []\n",
    "            try:\n",
    "                hashVector = [signatureMatrix[row][docNumber] for row in range (bandB * CONSTANT_ROW, ((bandB + 1) * CONSTANT_ROW))]\n",
    "            except:\n",
    "                hashVector = [signatureMatrix[row][docNumber] for row in range (bandB * CONSTANT_ROW, ((bandB) * CONSTANT_ROW))]\n",
    "                pass # I passed this statement and didn't wrote anything\n",
    "        \n",
    "            bucketId = \"\".join(map(str, hashVector))\n",
    "\n",
    "            if not bucketForBandB.get(bucketId):\n",
    "                bucketForBandB[bucketId] = set()\n",
    "\n",
    "            bucketForBandB[bucketId].add(docNumber)\n",
    "        bucketForBands[bandB] = bucketForBandB\n",
    "\n",
    "    print(\"[+] LSH Created Successfully\")\n",
    "\n",
    "    return bucketForBands # dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Function to perform LSH on CORPUS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performLSHcorpus():\n",
    "    '''\n",
    "    This is the function that call the relevant functions to perform LSH on the CORPUS of data. The Steps involved are:\n",
    "\n",
    "    1. Creating Shingles Vocabulary\n",
    "    2. Creating Shingles Matrix\n",
    "    3. Creating Signature Matrix using Min Hashing Technique\n",
    "    4. Performing LSH on Signature Matrix\n",
    "\n",
    "    This function return the bukcet formed by the LSH Function on corpus.\n",
    "    '''\n",
    "\n",
    "    # 1. Creating Shingles Vocabulary\n",
    "    nameOfDocuments, vocab = createShinglesVocab()\n",
    "\n",
    "    # 2. Creating Shingles Matrix\n",
    "    dictShinglesId = assignIdToShingles(vocab)\n",
    "    shingleMatrix = createShinglesMatrix(dictShinglesId)\n",
    "    # print(shingleMatrix)\n",
    "\n",
    "    # 3. Creating Signature Matrix\n",
    "    signatureMatrix = generateSignatureMatrix(shingleMatrix)\n",
    "    # print(signatureMatrix)\n",
    "\n",
    "    # 4. Performing LSH on Signature Matrix\n",
    "    corpusBucket = lsh(signatureMatrix)\n",
    "    print(corpusBucket)\n",
    "\n",
    "    return corpusBucket, dictShinglesId\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[.] Creating Shingles for the corpus!\n",
      "AUTO_1215E.2_final.txt: File has been shingled successfully!\n",
      "AUTO_7thEditionPolicy.txt: File has been shingled successfully!\n",
      "AUTO_AU127-1_final.txt: File has been shingled successfully!\n",
      "AUTO_Business Auto Coverage Form - CA 00_final.txt: File has been shingled successfully!\n",
      "AUTO_PERSONAL AUTO PP 00 01 06 98_final.txt: File has been shingled successfully!\n",
      "AUTO_PL-600003-87_final.txt: File has been shingled successfully!\n",
      "AUTO_insurance-pdf-NL-SPF-1.txt: File has been shingled successfully!\n",
      "PROPERTY_5105072011_booklet.txt: File has been shingled successfully!\n",
      "PROPERTY_BRIT-PO-Policy-Wording-May-2016-1_final.txt: File has been shingled successfully!\n",
      "PROPERTY_Liberty-SECT-Policy-Wording_final.txt: File has been shingled successfully!\n",
      "PROPERTY_Property-Owner-Policy-Wording_final.txt: File has been shingled successfully!\n",
      "PROPERTY_Residential-Property-Owners-Policy-Wording-1910_final.txt: File has been shingled successfully!\n",
      "PROPERTY_complete-property-owner-policy-wording-policies-incepting-or-renewing-from-010418-acom686-11_final.txt: File has been shingled successfully!\n",
      "PROPERTY_eSols Property Owners Commercial Policy Wording 2018 Policies which incepted before 01 July_final.txt: File has been shingled successfully!\n",
      "PROPERTY_rsa_property_owners_policy_wording.txt: File has been shingled successfully!\n",
      "[+] Shingles Vocabulary created successfully!\n",
      "[.] Creating Shingle Matrix...\n",
      "[+] Shingle Matrix Created Successfully!\n",
      "15\n",
      "[.] Processing Signature Matrix...\n",
      "[+] Signature Matrix Creating Successfull\n",
      "[.] LSH of Signature Matrix Started...\n",
      "[+] LSH Created Successfully\n",
      "{0: {'00': {0, 3, 4, 6, 7, 10, 11, 12}, '02': {8, 1, 9, 14}, '22': {2}, '05': {5}, '10': {13}}, 1: {'12': {0}, '120': {1}, '22': {2}, '50': {3}, '711': {4}, '51': {5}, '13': {6}, '01': {8, 7}, '00': {9, 13}, '03': {10, 12}, '02': {11}, '43': {14}}, 2: {'05': {0}, '92': {1}, '55': {2}, '10': {3}, '511': {4}, '54': {5}, '11': {6}, '00': {9, 10, 12, 7}, '03': {8, 13}, '04': {11}, '02': {14}}, 3: {'04': {0}, '21': {1}, '33': {2}, '83': {3}, '100': {4}, '104': {5}, '00': {10, 12, 13, 6}, '11': {7}, '01': {8, 9}, '10': {11}, '03': {14}}, 4: {'16': {0}, '21': {1}, '32': {2}, '20': {3}, '06': {4}, '265': {5}, '30': {6}, '02': {7}, '12': {8}, '31': {9}, '34': {10}, '13': {11}, '00': {12}, '01': {13, 14}}, 5: {'35': {0}, '63': {1}, '00': {2, 10, 11, 12, 13}, '06': {3}, '73': {4}, '20': {5}, '31': {6}, '02': {9, 7}, '05': {8}, '01': {14}}, 6: {'50': {0}, '01': {8, 1, 10, 12}, '21': {2}, '20': {3}, '153': {4}, '23': {5}, '54': {6}, '53': {7}, '04': {9}, '00': {11, 13, 14}}, 7: {'00': {0, 12, 7}, '05': {1}, '50': {2}, '80': {3, 5}, '105': {4}, '31': {6}, '20': {8}, '12': {9, 13}, '11': {10}, '02': {11}, '03': {14}}, 8: {'84': {0}, '02': {1, 2}, '54': {3}, '44': {4}, '46': {5}, '150': {6}, '12': {7}, '00': {8, 12}, '20': {9, 11, 14}, '10': {10}, '11': {13}}, 9: {'03': {0}, '20': {1}, '01': {2, 4, 5}, '31': {3}, '05': {6}, '10': {8, 14, 7}, '21': {9, 11}, '00': {10, 12}, '11': {13}}, 10: {'02': {0, 4}, '10': {1, 10, 12}, '05': {2}, '72': {3}, '62': {5}, '24': {6}, '20': {7}, '40': {8}, '11': {9, 13}, '00': {11}, '41': {14}}, 11: {'23': {0}, '02': {1}, '87': {2}, '85': {3}, '190': {4, 5}, '54': {6}, '20': {7}, '00': {8, 10, 11, 13}, '31': {9}, '10': {12, 14}}, 12: {'10': {0}, '21': {1, 4}, '81': {2}, '63': {3}, '01': {8, 10, 5, 14}, '91': {6}, '00': {9, 12, 13, 7}, '30': {11}}, 13: {'30': {0}, '31': {1}, '73': {2}, '40': {3, 4}, '50': {5}, '12': {6}, '00': {7}, '11': {8}, '01': {9, 10, 12, 13, 14}, '21': {11}}, 14: {'01': {0}, '00': {1, 10, 11, 12, 13}, '32': {2}, '42': {3}, '30': {4, 5, 6}, '11': {7}, '40': {8}, '10': {9, 14}}, 15: {'11': {0}, '51': {1}, '42': {2}, '10': {3, 6}, '117': {4}, '39': {5}, '32': {7}, '02': {8, 14}, '01': {9, 13}, '12': {10, 12}, '30': {11}}, 16: {'22': {0}, '16': {1}, '47': {2}, '106': {3}, '54': {4}, '44': {5}, '11': {6}, '10': {7}, '01': {8}, '42': {9}, '00': {10, 14}, '20': {11}, '02': {12, 13}}, 17: {'00': {0, 1, 2, 7}, '26': {3}, '13': {4, 12}, '43': {5}, '96': {6}, '31': {8}, '10': {9}, '20': {10, 11}, '12': {13}, '22': {14}}, 18: {'02': {0, 12}, '13': {1}, '151': {2}, '06': {3}, '204': {4}, '124': {5}, '78': {6}, '14': {7}, '40': {8}, '03': {9}, '01': {10}, '10': {11}, '21': {13}, '00': {14}}, 19: {'20': {0}, '35': {1}, '41': {2}, '62': {3}, '12': {4}, '51': {5}, '00': {8, 6, 14}, '21': {7}, '03': {9}, '02': {10, 12}, '10': {11, 13}}}\n"
     ]
    }
   ],
   "source": [
    "corpusBucket, dictShinglesId = performLSHcorpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Function to perform LSH on Query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performLSHquery(query, dictShinglesId):\n",
    "    '''\n",
    "    This function takes the query from the user and return the bukcet formed by the LSH Function on query. The Steps involved are:\n",
    "\n",
    "    1. Creating Shingles Vocabulary\n",
    "    2. Creating Shingles Matrix\n",
    "    3. Creating Signature Matrix using Min Hashing Technique\n",
    "    4. Performing LSH on Signature Matrix\n",
    "\n",
    "    This function return the bukcet formed by the LSH Function on query.\n",
    "    '''\n",
    "\n",
    "    # 1. Creating Shingles Vocabulary\n",
    "    # Already done while performing shingling of corpus \n",
    "    # Result in vocab set\n",
    "\n",
    "    # 2. Creating Shingles Matrix\n",
    "    shingleMatrix = createQueryMatrix(query, dictShinglesId)\n",
    "\n",
    "    # 3. Creating Signature Matrix\n",
    "    signatureMatrix = generateSignatureMatrix(shingleMatrix)\n",
    "\n",
    "    # 4. Performing LSH on Signature Matrix\n",
    "    queryBucket = lsh(signatureMatrix)\n",
    "\n",
    "\n",
    "    return queryBucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **INPUT QUERY FROM USER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"Enter your Query: \")\n",
    "print(query)\n",
    "matrixQueryShinglesId = createQueryMatrix(query, dictShinglesId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrixQueryShinglesId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryBucket = performLSHquery(query, dictShinglesId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryBucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusBucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Functions for Generating OUTPUT for User's Query**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Retriving Data of a specific document by using Doc ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataForDocumnetById(docId):\n",
    "    dataFolder = os.listdir(ASSETS_LOCATION)\n",
    "    dataFolder.sort()\n",
    "    \n",
    "    docName = dataFolder[docId]\n",
    "    docLocation = ASSETS_LOCATION + f'/{docName}'\n",
    "    filePtr = open(docLocation, 'r')\n",
    "    docData = filePtr.read()\n",
    "\n",
    "    return docName, docData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Finding similar document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def findSimilarDocs(corpusBucket ,queryBucket):\n",
    "#     \"\"\"\n",
    "#     This Function return the set of all the documents that are similar to the query input by the user\n",
    "#     \"\"\"\n",
    "#\n",
    "#     similarDocs = set()\n",
    "#\n",
    "#     for queryBand in queryBucket.keys():\n",
    "#         for (queryBucketIndex, queryBucketDocs) in queryBucket[queryBand].items():\n",
    "#             if queryBucketDocs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_docs(query_buckets, docs_buckets):\n",
    "    \"\"\"\n",
    "    Given the `docs_buckets` and the buckets `query_buckets` formed by the query\n",
    "    finds all the similar documents to the ones in `query_buckets` in `docs_buckets`.\n",
    "    \"\"\"\n",
    "    similar_docs = set()\n",
    "\n",
    "    for q_band_key in query_buckets.keys():\n",
    "        for q_bucket_idx, q_bucket_docs in query_buckets[q_band_key].items():\n",
    "            if q_bucket_docs:\n",
    "                if (\n",
    "                    q_band_key in docs_buckets\n",
    "                    and q_bucket_idx in docs_buckets[q_band_key]\n",
    "                ):\n",
    "                    similar_docs.update(docs_buckets[q_band_key][q_bucket_idx])\n",
    "    return similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qb = {\n",
    "#     1: {1: [0], 3: [0]},\n",
    "#     2: {4: [0]},\n",
    "# }\n",
    "\n",
    "# db = {\n",
    "#     1: {1: [0, 1, 2], 2: [3, 4]},\n",
    "#     2: {4: [6]}\n",
    "# }\n",
    "docIdList = find_similar_docs(queryBucket, corpusBucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docIdList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in docIdList:\n",
    "    print(getDataForDocumnetById(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
